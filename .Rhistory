dataset  <- fread("./datasets/paquete_premium_202011.csv")   #donde entreno
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/HT3220/", showWarnings = FALSE )
setwd("C:/Users/Julieta/OneDrive/MCD/segundo_año/Laboratorio_de_Implementacion_I\\labo\\exp\\HT3320\\")   #Establezco el Working Directory DEL EXPERIMENTO
archivo_log  <- "HT332.txt"
#leo si ya existe el log, para retomar en caso que se se corte el programa
GLOBAL_iteracion  <- 0
if( file.exists(archivo_log) )
{
tabla_log  <- fread( archivo_log )
GLOBAL_iteracion  <- nrow( tabla_log )
}
#La llamada con los parametros por default
x  <- list(  cp=          0.01,
minsplit=   20,
minbucket=   6,
maxdepth=   30
)
EstimarGanancia( x )
#Optimizacion Bayesiana de hiperparametros de  rpart
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rlist")
require("rpart")
require("parallel")
ksemilla_azar  <- 140293   #cambiar por la primer semilla
#------------------------------------------------------------------------------
#graba a un archivo los componentes de lista
#para el primer registro, escribe antes los titulos
loguear  <- function( reg, arch=NA, folder="./work/", ext=".txt", verbose=TRUE )
{
archivo  <- arch
if( is.na(arch) )  archivo  <- paste0( folder, substitute( reg), ext )
if( !file.exists( archivo ) )  #Escribo los titulos
{
linea  <- paste0( "fecha\t",
paste( list.names(reg), collapse="\t" ), "\n" )
cat( linea, file=archivo )
}
linea  <- paste0( format(Sys.time(), "%Y%m%d %H%M%S"),  "\t",     #la fecha y hora
gsub( ", ", "\t", toString( reg ) ),  "\n" )
cat( linea, file=archivo, append=TRUE )  #grabo al archivo
if( verbose )  cat( linea )   #imprimo por pantalla
}
#------------------------------------------------------------------------------
#particionar agrega una columna llamada fold a un dataset que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30), agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30
# particionar( data=dataset, division=c(1,1,1,1,1), agrupa=clase_ternaria, seed=semilla)   divide el dataset en 5 particiones
particionar  <- function( data, division, agrupa="", campo="fold", start=1, seed=NA )
{
if( !is.na( seed)  )   set.seed( seed )
bloque  <- unlist( mapply(  function(x,y) { rep( y, x ) }, division, seq( from=start, length.out=length(division) )  ) )
data[ , (campo) :=  sample( rep( bloque, ceiling(.N/length(bloque))) )[1:.N],
by= agrupa ]
}
#------------------------------------------------------------------------------
#fold_test  tiene el numero de fold que voy a usar para testear, entreno en el resto de los folds
#param tiene los hiperparametros del arbol
ArbolSimple  <- function( fold_test, data, param )
{
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",
data= data[ fold != fold_test, ],  #entreno en todo MENOS el fold_test que uso para testing
xval= 0,
control= param )
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,
data[ fold==fold_test, ],  #aplico el modelo sobre los datos de testing
type= "prob")   #quiero que me devuelva probabilidades
prob_baja2  <- prediccion[, "BAJA+2"]  #esta es la probabilidad de baja
#calculo la ganancia
ganancia_testing  <- data[ fold==fold_test ][ prob_baja2 > 1/60,
sum( ifelse( clase_ternaria=="BAJA+2", 59000, -1000 ) )]
return( ganancia_testing )  #esta es la ganancia sobre el fold de testing, NO esta normalizada
}
#------------------------------------------------------------------------------
ArbolesCrossValidation  <- function( data, param, qfolds, pagrupa, semilla )
{
divi  <- rep( 1, qfolds )  # generalmente  c(1, 1, 1, 1, 1 )  cinco unos
particionar( data, divi, seed=semilla, agrupa=pagrupa )  #particiono en dataset en folds
ganancias  <- mcmapply( ArbolSimple,
seq(qfolds), # 1 2 3 4 5
MoreArgs= list( data, param),
SIMPLIFY= FALSE,
mc.cores= 1 )   #se puede subir a qfolds si posee Linux o Mac OS
data[ , fold := NULL ]
#devuelvo la primer ganancia y el promedio
ganancia_promedio  <- mean( unlist( ganancias ) )   #promedio las ganancias
ganancia_promedio_normalizada  <- ganancia_promedio * qfolds  #aqui normalizo la ganancia
return( ganancia_promedio_normalizada )
}
#------------------------------------------------------------------------------
#esta funcion solo puede recibir los parametros que se estan optimizando
#el resto de los parametros, lamentablemente se pasan como variables globales
EstimarGanancia  <- function( x )
{
GLOBAL_iteracion  <<-  GLOBAL_iteracion + 1
xval_folds  <- 5
ganancia  <- ArbolesCrossValidation( dataset,
param= x, #los hiperparametros del arbol
qfolds= xval_folds,  #la cantidad de folds
pagrupa= "clase_ternaria",
semilla= ksemilla_azar )
#logueo
xx  <- x
xx$xval_folds  <-  xval_folds
xx$ganancia  <- ganancia
xx$iteracion <- GLOBAL_iteracion
loguear( xx,  arch= archivo_log )
return( ganancia )
}
#------------------------------------------------------------------------------
#Aqui empieza el programa
#setwd( "D:\\gdrive\\Austral2022R\\" )
setwd("C:\\Users\\Julieta\\OneDrive\\MCD\\segundo_año\\Laboratorio_de_Implementacion_I")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/paquete_premium_202011.csv")   #donde entreno
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/HT3220/", showWarnings = FALSE )
setwd("C:\\Users\\Julieta\\OneDrive\\MCD\\segundo_año\\Laboratorio_de_Implementacion_I\\labo\\exp\\HT3320\\")   #Establezco el Working Directory DEL EXPERIMENTO
archivo_log  <- "HT332.txt"
#leo si ya existe el log, para retomar en caso que se se corte el programa
GLOBAL_iteracion  <- 0
if( file.exists(archivo_log) )
{
tabla_log  <- fread( archivo_log )
GLOBAL_iteracion  <- nrow( tabla_log )
}
#La llamada con los parametros por default
x  <- list(  cp=          0.01,
minsplit=   20,
minbucket=   6,
maxdepth=   30
)
EstimarGanancia( x )
#Se utiliza el algoritmo Random Forest, creado por Leo Breiman en el año 2001
#Una libreria que implementa Rando Forest se llama  ranger
#La libreria esta implementada en lenguaje C y corre en paralelo, utiliza TODOS los nucleos del procesador
#Leo Breiman provenia de la estadistica y tenia "horror a los nulos", con lo cual el algoritmo necesita imputar nulos antes
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("ranger")
require("randomForest")  #solo se usa para imputar nulos
#Aqui se debe poner la carpeta de la computadora local
setwd("C:\\Users\\Julieta\\OneDrive\\MCD\\segundo_año\\Laboratorio_de_Implementacion_I")   #Establezco el Working Directory
#cargo los datos donde entreno
dtrain  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#imputo los nulos, ya que ranger no acepta nulos
#Leo Breiman, ¿por que le temias a los nulos?
dtrain  <- na.roughfix( dtrain )
#cargo los datos donde aplico el modelo
dapply  <- fread("./datasets/paquete_premium_202101.csv", stringsAsFactors= TRUE)
dapply[ , clase_ternaria := NULL ]  #Elimino esta columna que esta toda en NA
dapply  <- na.roughfix( dapply )  #tambien imputo los nulos en los datos donde voy a aplicar el modelo
#genero el modelo de Random Forest con la libreria ranger
#notar como la suma de muchos arboles contrarresta el efecto de min.node.size=1
param  <- list( "num.trees"=     2456,  #cantidad de arboles
"mtry"=             8,  #cantidad de variables que evalua para hacer un split  sqrt(ncol(dtrain))
"min.node.size"=  500,  #tamaño minimo de las hojas
"max.depth"=       14   # 0 significa profundidad infinita
)
set.seed(678923) #Establezco la semilla aleatoria
#para preparar la posibilidad de asignar pesos a las clases
#la teoria de  Maite San Martin
setorder( dtrain, clase_ternaria )  #primero quedan los BAJA+1, BAJA+2, CONTINUA
#genero el modelo de Random Forest llamando a ranger()
modelo  <- ranger( formula= "clase_ternaria ~ .",
data=  dtrain,
probability=   TRUE,  #para que devuelva las probabilidades
num.trees=     param$num.trees,
mtry=          param$mtry,
min.node.size= param$min.node.size,
max.depth=     param$max.depth
#,class.weights= c( 1,60, 1)  #siguiendo con la idea de Maite San Martin
)
#aplico el modelo recien creado a los datos del futuro
prediccion  <- predict( modelo, dapply )
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(prediccion$predictions[ ,"BAJA+2" ] > 1/60) ) ) #genero la salida
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2411/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2411/KA_411_002.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep="," )
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require( "data.table" )
dataset  <- fread( "~/buckets/b1/datasets/paquete_premium.csv.gz" )
# XGBoost  sabor tradicional ,  cambiando algunos de los parametros
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("xgboost")
#Aqui se debe poner la carpeta de la computadora local
setwd("D:\\gdrive\\Austral2022R\\")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
dataset[ , clase01 := ifelse( clase_ternaria=="BAJA+2", 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita XGBoost
dtrain  <- xgb.DMatrix( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
modelo  <- xgb.train( data= dtrain,
param= list( objective=       "binary:logistic",
max_depth=           4,
min_child_weight=   20 ),
nrounds= 40
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ]) )
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion)  ) #genero la salida
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2552/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2552/KA_552_001.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
# XGBoost  sabor tradicional ,  cambiando algunos de los parametros
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("xgboost")
#Aqui se debe poner la carpeta de la computadora local
setwd("D:\\gdrive\\Austral2022R\\")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
dataset[ , clase01 := ifelse( clase_ternaria=="BAJA+2", 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita XGBoost
dtrain  <- xgb.DMatrix( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
modelo  <- xgb.train( data= dtrain,
param= list( objective=       "binary:logistic",
max_depth=           4,
min_child_weight=   20 ),
nrounds= 40
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ]) )
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion > 1/60)  ) #genero la salida
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion)  ) #genero la salida
setorder(entrega, -Predicted)
entrega[Predicted:= 0]
entrega[1:11000, Predicted:= 1]
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2552/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2552/KA_552_002.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
# XGBoost  sabor tradicional ,  cambiando algunos de los parametros
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("xgboost")
#Aqui se debe poner la carpeta de la computadora local
setwd("D:\\gdrive\\Austral2022R\\")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
dataset[ , clase01 := ifelse( clase_ternaria=="BAJA+2", 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita XGBoost
dtrain  <- xgb.DMatrix( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
modelo  <- xgb.train( data= dtrain,
param= list( objective=       "binary:logistic",
max_depth=           4,
min_child_weight=   20 ),
nrounds= 40
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ]) )
#Genero la entrega para Kaggle
# entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
#                                  "Predicted"= prediccion > 1/60)  ) #genero la salida
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion)  ) #genero la salida
setorder(entrega, -Predicted)
entrega[,Predicted:= 0]
entrega[1:11000, Predicted:= 1]
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2552/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2552/KA_552_002.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
# XGBoost  sabor tradicional ,  cambiando algunos de los parametros
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("xgboost")
#Aqui se debe poner la carpeta de la computadora local
setwd("D:\\gdrive\\Austral2022R\\")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
dataset[ , clase01 := ifelse( clase_ternaria=="BAJA+2", 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita XGBoost
dtrain  <- xgb.DMatrix( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
modelo  <- xgb.train( data= dtrain,
param= list( objective=       "binary:logistic",
max_depth=           4,
min_child_weight=   20 ),
nrounds= 40
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ]) )
#Genero la entrega para Kaggle
# entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
#                                  "Predicted"= prediccion > 1/60)  ) #genero la salida
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion)  ) #genero la salida
setorder(entrega, -Predicted)
entrega[,Predicted:= 0]
entrega[1:14000, Predicted:= 1]
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2552/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2552/KA_552_002.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
# XGBoost  sabor tradicional ,  cambiando algunos de los parametros
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("xgboost")
#Aqui se debe poner la carpeta de la computadora local
#setwd("D:\\gdrive\\Austral2022R\\")   #Establezco el Working Directory
setwd("C:\\Users\\Julieta\\OneDrive\\MCD\\segundo_año\\Laboratorio_de_Implementacion_I")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
dataset[ , clase01 := ifelse( clase_ternaria=="BAJA+2", 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita XGBoost
dtrain  <- xgb.DMatrix( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
modelo  <- xgb.train( data= dtrain,
param= list( objective=       "binary:logistic",
max_depth=           4,
min_child_weight=   20 ),
nrounds= 40
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ]) )
#Genero la entrega para Kaggle
# entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
#                                  "Predicted"= prediccion > 1/60)  ) #genero la salida
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion)  ) #genero la salida
setorder(entrega, -Predicted)
entrega[,Predicted:= 0]
entrega[1:14000, Predicted:= 1]
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2552/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2552/KA_552_002.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
# XGBoost  sabor tradicional ,  cambiando algunos de los parametros
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("xgboost")
#Aqui se debe poner la carpeta de la computadora local
#setwd("D:\\gdrive\\Austral2022R\\")   #Establezco el Working Directory
setwd("C:\\Users\\Julieta\\OneDrive\\MCD\\segundo_año\\Laboratorio_de_Implementacion_I")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
dataset[ , clase01 := ifelse( clase_ternaria=="BAJA+2", 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita XGBoost
dtrain  <- xgb.DMatrix( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
modelo  <- xgb.train( data= dtrain,
param= list( objective=       "binary:logistic",
max_depth=           4,
min_child_weight=   20 ),
nrounds= 40
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ]) )
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion > 0.0160870076191375)  ) #genero la salida
# entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
#                                  "Predicted"= prediccion)  ) #genero la salida
#
# setorder(entrega, -Predicted) #ordeno por probabilidad
#
# entrega[,Predicted:= 0]
# entrega[1:14000, Predicted:= 1]
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2552/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2552/KA_552_003.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
install.packages("yaml")
# LightGBM  cambiando algunos de los parametros
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("lightgbm")
#Aqui se debe poner la carpeta de la computadora local
#setwd("D:\\gdrive\\Austral2022R\\")   #Establezco el Working Directory
setwd("C:\\Users\\Julieta\\OneDrive\\MCD\\segundo_año\\Laboratorio_de_Implementacion_I")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
dataset[ , clase01 := ifelse( clase_ternaria=="BAJA+2", 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
modelo  <- lgb.train( data= dtrain,
param= list( objective=        "binary",
num_iterations=     566,   #40
num_leaves=         1011,  #64
min_data_in_leaf=   3726,  #3000
seed=             140293)
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ]) )
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= prediccion > 0.016896266)  ) #genero la salida
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2512/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2512/KA_512_003.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
#ahora imprimo la importancia de variables
tb_importancia  <-  as.data.table( lgb.importance(modelo) )
archivo_importancia  <- "./labo/exp/KA2512/512_importancia_003.txt"
fwrite( tb_importancia,
file= archivo_importancia,
sep= "\t" )
